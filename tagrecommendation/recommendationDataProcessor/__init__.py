#
# Freesound is (c) MUSIC TECHNOLOGY GROUP, UNIVERSITAT POMPEU FABRA
#
# Freesound is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# Freesound is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Authors:
#     See AUTHORS file.
#

from tagrecommendation_settings import RECOMMENDATION_TMP_DATA_DIR, RECOMMENDATION_DATA_DIR
import fileinput, sys
from utils import saveToJson, mtx2npy, loadFromJson
from numpy import save, load, where, in1d
from math import sqrt
from pysparse import spmatrix
from communityDetection import CommunityDetector


class RecommendationDataProcessor:
    '''
    This class has methods to generate all the files that the tag recommendation systems needs to recommend tags.
    To generate these files the data processor needs a .tas file with the tag association information from freesound.
    The .tas file must be named with the name of the corresponding database (ex: FREESOUND201309)
    The .tas file must have the following form:

    sound1id     user1id      tag1    tag2    tagN
    sound2id     user2id      tag1    tag2    tagN
    ...
    (separations are tabs, each line shows all tags associated so a single sound)

    The files that are generated by the system are:
    (for every sound class: Soundscape, Music, Fx, Samples, Speech)
    [[DATABASE]]_[[CLASSNAME]]_SIMILARITY_MATRIX_cosine_SUBSET.npy
    [[DATABASE]]_[[CLASSNAME]]_SIMILARITY_MATRIX_cosine_SUBSET_TAG_NAMES.npy
    '''

    verbose = None

    def __init__(self, verbose=True):
        self.verbose = verbose

    def __repr__(self):
        return "RecommendationDataProcessor instance"

    def tas_to_association_matrix(self, filename, tag_threshold=0, line_limit=1000000000):
        '''
        filename = complete path where the .tas file is located
        '''

        # Get tags from file
        ts = []
        idx = 0
        n_original_associations = 0
        if self.verbose:
            print "Reading file for tags...",
        for line in fileinput.input([filename]):
            line_parts = line.split('\t')
            line_parts[-1] = line_parts[-1][:-1]
            tags = line_parts[2:]
            ts += tags
            n_original_associations += len(tags)

            idx += 1
            if idx > line_limit:
                break
        fileinput.close()
        if self.verbose:
            print "done!"

        # Compute tag ocurrences after loading the file
        tag_occurrences = dict()
        unique_ts = list(set(ts))
        for id, t in enumerate(unique_ts):
            tag_occurrences[t] = ts.count(t)

            if self.verbose:
                sys.stdout.write("\rComputing tag occurrences %.2f%%"%(float(100*(id+1))/len(unique_ts)))
                sys.stdout.flush()
        print ""
        tags = []
        tags_ids = []
        for id, t in enumerate(unique_ts):

            if tag_occurrences[t] >= tag_threshold:
                tags.append(t)
                tags_ids.append(id)

            if self.verbose:
                sys.stdout.write("\rFiltering tags %.2f%%"%(float(100*(id+1))/len(unique_ts)))
                sys.stdout.flush()

        nTags = len(tags)
        if self.verbose:
            print ""
            print "\tOriginal number of tags: " + str(len(unique_ts))
            print "\tTags after filtering: " + str(nTags)

        # Generate resource-tags dictionary only with filtered tags
        if self.verbose:
            print "Reading file for resources...",
        sys.stdout.flush()
        res_tags = {}
        res_user = {}
        res_tags_no_filt = {}
        idx = 0
        n_filtered_associations = 0
        for line in fileinput.input([filename]):
            line_parts = line.split('\t')
            line_parts[-1] = line_parts[-1][:-1]
            resource = line_parts[0]
            user = line_parts[1]
            assigned_tags = line_parts[2:]
            assigned_tags_filt = list(set(assigned_tags).intersection(set(tags)))
            res_tags_no_filt[resource] = assigned_tags
            res_user[resource] = user
            if len(assigned_tags_filt) > 0:
                res_tags[resource] = assigned_tags_filt
                n_filtered_associations += len(assigned_tags_filt)

            idx += 1
            if idx > line_limit:
                break
        fileinput.close()
        resources = res_tags.keys()
        nResources = len(resources)
        resources_ids = range(0,nResources)
        if self.verbose:
            print "done!"

        # Generate assocoation matrix
        if self.verbose:
            print "\tOriginal number of associations: " + str(n_original_associations)
            print "\tAssociations after filtering: " + str(n_filtered_associations)

        if self.verbose:
            print 'Creating empty array of ' + str(nResources) + ' x ' + str(nTags) + '...',
        M = spmatrix.ll_mat(nResources, nTags)
        if self.verbose:
            print 'done!'

        done = 0
        for r_id in resources:
            for t in res_tags[r_id]:
                M[resources.index(r_id),tags.index(t)] = 1
                done += 1
                if self.verbose:
                    sys.stdout.write("\rGenerating association matrix %.2f%%" % (float(100*done)/n_filtered_associations))
                    sys.stdout.flush()
        if self.verbose:
            print ""

        # Save data
        if self.verbose:
            print "Saving association matrix, resource ids, tag ids and tag names"
        filename = filename.split("/")[-1]  # rename filename to save data in the right place
        M.export_mtx(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_ASSOCIATION_MATRIX.mtx')
        save(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_RESOURCE_IDS.npy',resources)
        save(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_TAG_IDS.npy',tags_ids)
        save(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_TAG_NAMES.npy',tags)
        saveToJson(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_RESOURCES_TAGS.json',res_tags, verbose = self.verbose)
        #saveToJson(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_RESOURCES_TAGS_NO_FILTER.json',res_tags_no_filt, verbose = self.verbose)
        #saveToJson(RECOMMENDATION_TMP_DATA_DIR + filename[:-4] + '_RESOURCES_USER.json',res_user, verbose = self.verbose)

        return filename[:-4]


    def association_matrix_to_similarity_matrix(self,
                                                metric="cosine",
                                                dataset="FREESOUND",
                                                save_sim=False,
                                                training_set=None,
                                                out_name_prefix="",
                                                is_general_recommender=False):

        if self.verbose:
            print "Loading association matrix and tag names, ids files..."
        try:
            M = spmatrix.ll_mat_from_mtx(RECOMMENDATION_TMP_DATA_DIR + dataset + "_ASSOCIATION_MATRIX.mtx")
            resource_ids = load(RECOMMENDATION_TMP_DATA_DIR + dataset + "_RESOURCE_IDS.npy")
            tag_names = load(RECOMMENDATION_TMP_DATA_DIR + dataset + "_TAG_NAMES.npy")
        except Exception:
            raise Exception("Error loading association matrix and tag names, ids data")

        if metric not in ['cosine', 'binary', 'coocurrence', 'jaccard']:
            raise Exception("Wrong similarity metric specified")

        if self.verbose:
            print "Computing similarity matrix from a resource subset of the whole association matrix..."
        # Get index of resources to train (usable index for M)
        resource_id_positions = where(in1d(resource_ids, training_set, assume_unique=True))[0]

        # Matrix multiplication (only taking in account resources in training set and ALL tags)
        MM = spmatrix.dot(M[resource_id_positions, :], M[resource_id_positions, :])

        # Get similarity matrix
        sim_matrix = spmatrix.ll_mat(MM.shape[0],MM.shape[0])
        non_zero_index = MM.keys()
        for index in non_zero_index:
            if metric == 'cosine':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]] * (1 / (sqrt(MM[index[0], index[0]]) * sqrt(MM[index[1], index[1]])))
            elif metric == 'coocurrence':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]]
            elif metric == 'binary':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]]/MM[index[0], index[1]]
            elif metric == 'jaccard':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]] * (1 / (MM[index[0], index[0]] + MM[index[1], index[1]] - MM[index[0], index[1]]))

        # Clean out similarity matrix (clean tags that are not used)
        tag_positions = []
        for i in range(0, sim_matrix.shape[0]):
            if sim_matrix[i, i] != 0.0:
                tag_positions.append(i)

        # Transform sparse similarity matrix to npy format
        sim_matrix_npy = mtx2npy(sim_matrix[tag_positions,tag_positions])
        tag_names_sim_matrix = tag_names[tag_positions]

        if save_sim:
            if not is_general_recommender:
                # Save sim
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_%s_SIMILARITY_MATRIX_" % out_name_prefix + metric + "_SUBSET.npy"
                if self.verbose:
                    print "Saving to " + path + "..."
                save(path, sim_matrix_npy)

                # Save tag names
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_%s_SIMILARITY_MATRIX_" % out_name_prefix + metric + "_SUBSET_TAG_NAMES.npy"
                if self.verbose:
                    print "Saving to " + path + "..."
                save(path, tag_names_sim_matrix)
            else:
                # Save sim
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_SIMILARITY_MATRIX_" + metric + ".npy"
                if self.verbose:
                    print "Saving to " + path + "..."
                save(path, sim_matrix_npy)

                # Save tag names
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_SIMILARITY_MATRIX_" + metric + "_TAG_NAMES.npy"
                if self.verbose:
                    print "Saving to " + path + "..."
                save(path, tag_names_sim_matrix)

        return {'SIMILARITY_MATRIX': sim_matrix_npy, 'TAG_NAMES': tag_names_sim_matrix}

    def process_tag_recommendation_data(self,
                                        filename=None,
                                        resources_limit=None,
                                        tag_threshold=10,
                                        line_limit=99999999999999,
                                        recompute_all_classes=False,
                                        similarity_metric="cosine"):

        # Process tas file and turn into association matrix and derived files
        database_name = self.tas_to_association_matrix(filename, tag_threshold=tag_threshold, line_limit=line_limit)

        print "Loading community detector..."
        cd = CommunityDetector(verbose=False, PATH=RECOMMENDATION_DATA_DIR + "Classifier")
        print cd

        # Classify existing resources
        print "Classifying resources..."
        resources_tags = loadFromJson(RECOMMENDATION_TMP_DATA_DIR + database_name + '_RESOURCES_TAGS.json')
        instances_ids = resources_tags.keys()
        try:
            resource_class = loadFromJson(RECOMMENDATION_TMP_DATA_DIR + 'CLASSIFIED_RESOURCES.json')
        except Exception, e:
            resource_class = dict()

        for count, id in enumerate(instances_ids):
            if not recompute_all_classes:
                if id not in resource_class:
                    resource_class[id] = cd.detectCommunity(input_tags=resources_tags[id])
            else:
                resource_class[id] = cd.detectCommunity(input_tags=resources_tags[id])

            if self.verbose:
                sys.stdout.write("\rClassifying resources... %.2f%%"%(float(100*(count+1))/len(instances_ids)))
                sys.stdout.flush()

        print ""

        print "\nComputing data for general recommender..."
        self.association_matrix_to_similarity_matrix(
            dataset=database_name,
            training_set=instances_ids[0:resources_limit],
            save_sim=True,
            is_general_recommender=True,
            metric=similarity_metric,
        )

        print "\nComputing data for class recommenders..."
        instance_id_class = []
        distinct_classes = []
        for count, instance_id in enumerate(instances_ids):
            class_id = resource_class[instance_id]
            instance_id_class.append([instance_id, class_id])

            if class_id not in distinct_classes:
                distinct_classes.append(class_id)

        print distinct_classes

        for collection_id in distinct_classes:
            print "\nComputing recommender for collection %s..." % collection_id

            # All resources from the training set classified as the selected category
            # (instead of all manually labeled)
            training_ids = []
            for instance in instance_id_class:
                if instance[1] == collection_id:
                    training_ids.append(instance[0])
            # Add limit
            training_ids = training_ids[0:resources_limit]

            if len(training_ids) < 1:
                raise Exception("Too less training ids for collection %s" % collection_id)

            self.association_matrix_to_similarity_matrix(
                dataset=database_name,
                training_set=training_ids,
                save_sim=True,
                out_name_prefix=collection_id,
                is_general_recommender=False,
                metric=similarity_metric,
            )

        # TODO: move files to the tag recommendation directory and clean intermediate files from temp directory
            

'''
from recommendationDataProcessor import RecommendationDataProcessor
rdp = RecommendationDataProcessor()
filename= "/Users/frederic/SMC/Freesound/freesound-tagrecommendation/tmp/FREESOUND.tas"
rdp.process_tag_recommendation_data(filename=filename,tag_threshold=10,line_limit=1000)
'''